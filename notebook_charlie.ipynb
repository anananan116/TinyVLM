{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TinyVLM\n",
    "\n",
    "## Data Exploration and Initial Preprocessing\n",
    "\n",
    "### Data Exploration\n",
    "\n",
    "##### Data\n",
    "\n",
    "- [Images used for training with descriptions](https://huggingface.co/datasets/BAAI/CapsFusion-120M)\n",
    "\n",
    "General Info on this Data:\n",
    "\n",
    "- This dataset provides over 13 million image links, but we are scaling down. We downloaded the first 5 million rows of the dataset, and of these we will only use the rows where the image link gives a successful response code. For the purpose of data exploration, we are just using the first 100,000 rows.\n",
    "- All initial images are not uniform in any regard, however during preprocessing, all images will be cropped (either center cropped or padded)\n",
    "- 3 different descriptions for each images as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_url</th>\n",
       "      <th>capsfusion</th>\n",
       "      <th>identifier</th>\n",
       "      <th>original_width</th>\n",
       "      <th>original_height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://ih3.redbubble.net/image.12080909.2547/f...</td>\n",
       "      <td>The Lego minifigure, known as Minifig [Rainbow...</td>\n",
       "      <td>515623f9-0f34-49b1-be2c-253636badaf6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://cdn.shopify.com/s/files/1/2161/7557/pr...</td>\n",
       "      <td>The Chilly Grip H2O Waterproof Thermal Lined, ...</td>\n",
       "      <td>ccb9ec94-d0d4-4384-b728-a8d9ad150aa6</td>\n",
       "      <td>439.0</td>\n",
       "      <td>480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://i0.wp.com/www.ladycarehealth.com/wp-con...</td>\n",
       "      <td>Abdominal cramping is a common sign of pregnan...</td>\n",
       "      <td>642a0186-bbc4-4244-b87b-617d4ea350ed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://cdn.shopify.com/s/files/1/2986/1514/pr...</td>\n",
       "      <td>This lovely 1930s Velveteen Burgundy Half Slee...</td>\n",
       "      <td>9be0e45a-bdd0-410d-b825-77b3781eaa84</td>\n",
       "      <td>800.0</td>\n",
       "      <td>1156.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://cdn.shopify.com/s/files/1/2169/9777/pr...</td>\n",
       "      <td>The Bubblegum Divas Store offers a Little Yell...</td>\n",
       "      <td>b75f96ac-c759-415c-91de-4502da4a3156</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           image_url  \\\n",
       "0  http://ih3.redbubble.net/image.12080909.2547/f...   \n",
       "1  https://cdn.shopify.com/s/files/1/2161/7557/pr...   \n",
       "2  http://i0.wp.com/www.ladycarehealth.com/wp-con...   \n",
       "3  https://cdn.shopify.com/s/files/1/2986/1514/pr...   \n",
       "4  https://cdn.shopify.com/s/files/1/2169/9777/pr...   \n",
       "\n",
       "                                          capsfusion  \\\n",
       "0  The Lego minifigure, known as Minifig [Rainbow...   \n",
       "1  The Chilly Grip H2O Waterproof Thermal Lined, ...   \n",
       "2  Abdominal cramping is a common sign of pregnan...   \n",
       "3  This lovely 1930s Velveteen Burgundy Half Slee...   \n",
       "4  The Bubblegum Divas Store offers a Little Yell...   \n",
       "\n",
       "                             identifier  original_width  original_height  \n",
       "0  515623f9-0f34-49b1-be2c-253636badaf6             NaN              NaN  \n",
       "1  ccb9ec94-d0d4-4384-b728-a8d9ad150aa6           439.0            480.0  \n",
       "2  642a0186-bbc4-4244-b87b-617d4ea350ed             NaN              NaN  \n",
       "3  9be0e45a-bdd0-410d-b825-77b3781eaa84           800.0           1156.0  \n",
       "4  b75f96ac-c759-415c-91de-4502da4a3156           100.0            100.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('data/image_metadata_0.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of our Dataframe(100000, 5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of our Dataframe{df.shape}')\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also using the following datasets for instruction tuning. The first dataset below contains questions, and second contains answers.\n",
    "\n",
    "- [Instruction Tuning: VQA](https://visualqa.org/download.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>question</th>\n",
       "      <th>question_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>458752</td>\n",
       "      <td>What is this photo taken looking through?</td>\n",
       "      <td>458752000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>458752</td>\n",
       "      <td>What position is this man playing?</td>\n",
       "      <td>458752001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>458752</td>\n",
       "      <td>What color is the players shirt?</td>\n",
       "      <td>458752002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>458752</td>\n",
       "      <td>Is this man a professional baseball player?</td>\n",
       "      <td>458752003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>262146</td>\n",
       "      <td>What color is the snow?</td>\n",
       "      <td>262146000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id                                     question  question_id\n",
       "0    458752    What is this photo taken looking through?    458752000\n",
       "1    458752           What position is this man playing?    458752001\n",
       "2    458752             What color is the players shirt?    458752002\n",
       "3    458752  Is this man a professional baseball player?    458752003\n",
       "4    262146                      What color is the snow?    262146000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_q.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_type</th>\n",
       "      <th>multiple_choice_answer</th>\n",
       "      <th>answers</th>\n",
       "      <th>image_id</th>\n",
       "      <th>answer_type</th>\n",
       "      <th>question_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is this</td>\n",
       "      <td>net</td>\n",
       "      <td>[{'answer': 'net', 'answer_confidence': 'maybe...</td>\n",
       "      <td>458752</td>\n",
       "      <td>other</td>\n",
       "      <td>458752000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what</td>\n",
       "      <td>pitcher</td>\n",
       "      <td>[{'answer': 'pitcher', 'answer_confidence': 'y...</td>\n",
       "      <td>458752</td>\n",
       "      <td>other</td>\n",
       "      <td>458752001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what color is the</td>\n",
       "      <td>orange</td>\n",
       "      <td>[{'answer': 'orange', 'answer_confidence': 'ye...</td>\n",
       "      <td>458752</td>\n",
       "      <td>other</td>\n",
       "      <td>458752002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is this</td>\n",
       "      <td>yes</td>\n",
       "      <td>[{'answer': 'yes', 'answer_confidence': 'yes',...</td>\n",
       "      <td>458752</td>\n",
       "      <td>yes/no</td>\n",
       "      <td>458752003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what color is the</td>\n",
       "      <td>white</td>\n",
       "      <td>[{'answer': 'white', 'answer_confidence': 'yes...</td>\n",
       "      <td>262146</td>\n",
       "      <td>other</td>\n",
       "      <td>262146000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       question_type multiple_choice_answer  \\\n",
       "0       what is this                    net   \n",
       "1               what                pitcher   \n",
       "2  what color is the                 orange   \n",
       "3            is this                    yes   \n",
       "4  what color is the                  white   \n",
       "\n",
       "                                             answers  image_id answer_type  \\\n",
       "0  [{'answer': 'net', 'answer_confidence': 'maybe...    458752       other   \n",
       "1  [{'answer': 'pitcher', 'answer_confidence': 'y...    458752       other   \n",
       "2  [{'answer': 'orange', 'answer_confidence': 'ye...    458752       other   \n",
       "3  [{'answer': 'yes', 'answer_confidence': 'yes',...    458752      yes/no   \n",
       "4  [{'answer': 'white', 'answer_confidence': 'yes...    262146       other   \n",
       "\n",
       "   question_id  \n",
       "0    458752000  \n",
       "1    458752001  \n",
       "2    458752002  \n",
       "3    458752003  \n",
       "4    262146000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_a.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Steps\n",
    "\n",
    "For preprocessing, we plan on doing the following:\n",
    "\n",
    "- Downloading only images in which gives a successful response code (rows in the dataset corresponding to images without a successful response code will be disregarded)\n",
    "- Cropping all the images to a desired 128 x 128 dimension\n",
    "- Normalization is likely not needed, however will perform when needed\n",
    "- Classification of the data, classifying each of the features\n",
    "- Encorporate image descriptions to the desired images for training\n",
    "- Prepare questions and answers for images to do instruction Tuning to the LLM pre-train model\n",
    "\n",
    "Our dataset consists of images with a wide variety of aspect ratios. Some images are already square or nearly square, whereas others have extreme aspect ratios (very narrow/wide). To account for this, we will set an aspect ratio threshold of 0.6, where aspect ratio is defined as the minimum of the width and height over the maximum of the width and height. For images with an aspect ratio greater than or equal to 0.6, we will center crop the image, and images with an aspect ratio less than 0.6 will be padded. An example of our preprocessing for a single image is as follows: Say we have a very narrow image with a height of 400px and a width of 100px. The image will be padded to make it square, meaning black bars will be added to the left and right of the image, each one having a height of 400px and a width of 150px. The image will then be downscaled to 128x128.\n",
    "\n",
    "Our function for preprocessing an image is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(self, img):\n",
    "    \"\"\"\n",
    "    Adaptively choose preprocessing method based on aspect ratio\n",
    "    \"\"\"\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    \n",
    "    width, height = img.size\n",
    "    aspect_ratio = min(width, height) / max(width, height)\n",
    "    \n",
    "    # Track which method was used\n",
    "    if aspect_ratio >= self.aspect_ratio_threshold:\n",
    "        # Use center crop for images with good aspect ratio\n",
    "        for transform in self.crop_transforms:\n",
    "            img = transform(img)\n",
    "        with self.stats_lock:\n",
    "            self.stats['crop_count'] += 1\n",
    "    else:\n",
    "        # Use padding for images with extreme aspect ratios\n",
    "        img = self.resize_and_pad(img)\n",
    "        with self.stats_lock:\n",
    "            self.stats['pad_count'] += 1\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use CLIP labels in order to encode our images to certain text labels from our CLIP labels that were created. In the process of making the CLIP labels, we had a few example results with the encoded result from it. From here we would pass in these samples into GPT in order to help generate more samples given a certain categorical inputs. From here, we are able to expand our CLIP labels from 10 for each individual categories we had, to 50-100 new labels for each different categories. Here, we will be able to pass this in for our pretraining to encode our images for classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparing for instruction tuning\n",
    "\n",
    "1. Download questions and answers from the dataset\n",
    "2. Create DataFrame from json data files which include questions and answers\n",
    "3. Combine questions and answers and create a new DataFrame according to the image_id and answer_id\n",
    "4. Create answers into a complete sentences\n",
    "5. Add tags to indicate system prompt, user questions, and answers from the model\n",
    "6. Combine system prompt, user questions, and answers into one col and label them with corrsponding image_id\n",
    "7. Output the data as csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:29:29) \n[Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5486444823919bf5533df1e16296b0e50a26ea647816ff0bc099daecd9efa1ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
