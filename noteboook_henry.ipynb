{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TinyVLM\n",
    "\n",
    "## Data Exploration and Initial Preprocessing\n",
    "\n",
    "### Data Exploration\n",
    "\n",
    "##### Data\n",
    "\n",
    "- [Images used for training with descriptions](https://huggingface.co/datasets/BAAI/CapsFusion-120M)\n",
    "- [Instruction Tuning #1](https://textvqa.org/dataset/)\n",
    "- [Instruction Tuning #2](https://visualqa.org/download.html)\n",
    "\n",
    "General Info on our Data:\n",
    "\n",
    "- Our dataset provides over 13 million image links, but we are scaling down. We downloaded the first 5 million rows of the dataset, and of these we will only use the rows where the image link gives a successful response code.\n",
    "- All initial images are not uniform in any regard, however during preprocessing, all images will be cropped\n",
    "- 3 different descriptions for each images as features\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "For preprocessing, we plan on doing the following:\n",
    "\n",
    "- Downloading only images in which gives a successful response code (rows in the dataset corresponding to images without a successful response code will be disregarded)\n",
    "- Cropping all the images to a desired 128 x 128 dimension\n",
    "- Normalization is likely not needed, however will perform when needed\n",
    "- Classification of the data, classifying each of the features\n",
    "- Encorporate image descriptions to the desired images for training\n",
    "- Prepare questions and answers for images to do instruction Tuning to the LLM pre-train model\n",
    "\n",
    "Our dataset consists of images with a wide variety of aspect ratios. Some images are already square or nearly square, whereas others have extreme aspect ratios (very narrow/wide). To account for this, we will set an aspect ratio threshold of 0.6, where aspect ratio is defined as the minimum of the width and height over the maximum of the width and height. For images with an aspect ratio greater than or equal to 0.6, we will center crop the image, and images with an aspect ratio less than 0.6 will be padded. An example of our preprocessing for a single image is as follows: Say we have a very narrow image with a height of 400px and a width of 100px. The image will be padded to make it square, meaning black bars will be added to the left and right of the image, each one having a height of 400px and a width of 150px. The image will then be downscaled to 128x128.\n",
    "\n",
    "\n",
    "We will also use CLIP labels in order to encode our images to certain text labels from our CLIP labels that were created. In the process of making the CLIP labels, we had a few example results with the encoded result from it. From here we would pass in these samples into GPT in order to help generate more samples given a certain categorical inputs. From here, we are able to expand our CLIP labels from 10 for each individual categories we had, to 50-100 new labels for each different categories. Here, we will be able to pass this in for our pretraining to encode our images for classification.\n",
    "\n",
    "### Data preparing for instruction tuning\n",
    "\n",
    "1. Download questions and answers from the dataset\n",
    "2. Create DataFrame from json data files which include questions and answers\n",
    "3. Combine questions and answers and create a new DataFrame according to the image_id and answer_id\n",
    "4. Create answers into a complete sentences\n",
    "5. Add tags to indicate system prompt, user questions, and answers from the model\n",
    "6. Combine system prompt, user questions, and answers into one col and label them with corrsponding image_id\n",
    "7. Output the data as csv file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('data/image_metadata_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of our Dataframe(100000, 5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of our Dataframe{df.shape}')\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:29:29) \n[Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5486444823919bf5533df1e16296b0e50a26ea647816ff0bc099daecd9efa1ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
